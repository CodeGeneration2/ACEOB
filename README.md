# Measuring Code Efficiency Optimization Capabilities with ACEOB

## A B S T R A C T

As the gains from Moore’s Law gradually diminish, the performance and efficiency of software
have become increasingly critical. However, optimizing code efficiency remains a formidable
challenge, even for professional programmers. Surprisingly, research on code efficiency optimization is relatively sparse, and rigorously assessing models’ abilities to optimize code efficiency is particularly challenging. Furthermore, recent large-scale language models frequently
generate “slow positive”, failing to meet the time requirements of algorithms. In response to
these challenges, we first explored the “code patterns” encountered by models in the training
dataset, conducting an in-depth investigation and analysis of the content of the training dataset,
namely, human-written code. Second, we defined the task of code efficiency optimization and
introduced the Automatic Code Efficiency Optimization Benchmark (ACEOB), a standard for
evaluating code efficiency optimization capabilities. This benchmark contains 95,522 pairs of
efficient-inefficient codes submitted by human programmers to coding competitions. To our
knowledge, the ACEOB dataset is the first specifically dedicated to Python code efficiency
optimization. Then, to evaluate the model’s ability to optimize code efficiency, we proposed
two novel metrics: Isomorphic Optimal Comparison CodeBLEU (IOCCB) and Normalized
Performance Index (NPI), to assess the efficiency of the code generated by the models. Lastly,
we tested the performance of several state-of-the-art code generation models, such as PolyCoder
and CodeT5, after fine-tuning them on the ACEOB dataset. Additionally, we introduced the
NPI filter, and we found that the performance of all models improved after applying this filter.
However, we found that the current cutting-edge AI model—ChatGPT—does not perform ideally
in the task of code efficiency optimization. 


## ACEOB-Ori Dataset (https://github.com/CodeGeneration2/GEC-CG-DataSet)

We utilized the data collected in Section 5.2 to systematically assemble the ACEOB-Ori dataset.
The Automatic Code Efficiency Optimization Benchmark Original (ACEOB-Ori) comprises:

• A total of 5,262 problems. Each problem includes a Natural Language description (incorporating the problem
statement, time/space constraints, I/O description, and I/O unit test cases/explanation), the URL of the problem
source, and the URL of the code source. These problems are challenging and complex, given that the average
length of their natural language descriptions is 578 words. Furthermore, we have organized the statistical data
from Section 2 and included it here.

• 901,038 code entries. These code entries were uniformly sampled from all codes on Codeforces, based on their
execution time. Each code entry contains information about its running time, used space, and NPI score (refer
to Section 6.4).

• I/O unit tests. These are split into public and hidden types. The publicly available I/O unit tests, just like NL,
serve as inputs. The hidden I/O unit tests are utilized to assess the functionality of the code, determining its
capability to accomplish tasks. The public I/O unit tests typically range from 1 to 2, while each problem averages
47 hidden I/O unit tests.

• 36 types of algorithm labels. These algorithm labels represent the recommended algorithmic strategies for
solving the given problems, with examples including math, geometry, and greedy. On average, each problem is
associated with 2.5 algorithm labels.

• 28 levels of difficulty categories. They range from the simplest entry-level difficulty (level 0) to the most
challenging level (level 27).

## The predictor

To calculate the RES scores for partial codes, we trained a code running time predictor using CodeT5-base to predict the running time of the code. The fine-tuning settings for this predictor were the same as those for the previously mentioned CodeT5 models. The predictor is fine-tuned using the GEC dataset. Unlike the GEC task, the “input feature” of the predictor refers to code, and the “label” used for gradient propagation is the code execution time (obtained from the codeforces website).

The model parameters for the runtime predictor are here https://drive.google.com/file/d/1YRXfFuzHq1TsNMpAivPYaUySsbSV2Nmt/view?usp=sharing.
