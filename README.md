# Efficient-Code-Generation-with-E-Code

## DataSet
  Since efficient code generation is a new branch that is opened for code generation, we curate a new dataset of efficient code generation programming problems called ECG for fine-tuning and evaluation. Accordingly, our model is fine-tuned on the ECG dataset. 
  
  The ECG draws on the APPS dataset (Hendrycks et al., 2021) and the CodeContests dataset (Li et al., 2022). We describe the dataset creation process and creative ideas in detail in Readme for DataSet folder.

  Although we developed the ECG dataset to perform efficient code generation, the ECG dataset is an exhaustive dataset that can be applied to different tasks. Therefore, we derived three datasets from this feature of the ECG dataset that can be applied to different specific code processing tasks to fill the gap of other code processing-oriented datasets.
  
  Among the ECG datasets our model uses for efficient code generation, we derive three datasets from them: ECG-CG, ECG-mini, and ECG-clone. We present each dataset in Readme for DataSet folder, respectively.
  
See the Readme file in [DataSet/README](https://github.com/CodeGeneration2/Efficient-Code-Generation-with-E-Code/main/DataSet/README.md)


## How to Use
The training prediction code is in the [Training prediction code](https://github.com/CodeGeneration2/Efficient-Code-Generation-with-E-Code/tree/main/Training%20prediction%20code) folder, and the details of how to use it are in the [Training prediction code/README](https://github.com/CodeGeneration2/Efficient-Code-Generation-with-E-Code/blob/main/Training%20prediction%20code/README.md)


## Diagrammatic figure
The diagrammatic figure is in the [Diagrammatic figure](https://github.com/CodeGeneration2/Efficient-Code-Generation-with-E-Code/tree/main/Diagrammatic%20figure) folder.


## Generated code has been predicted
In the Efficient-Code-Generation-with-E-Code work, the authors use a fine-tuned pre-trained model to predict a range of codes to be generated. 
Due to the need for comparative experiments, three code generation models are available. 
The three code generation models are E-code 350M, GPT-Neo 125M, and No expert group E-code 350M. 
We use each of the three fine-tuned code generation models to generate codes. 
Below we have [the code generated by the three fine-tuned code generation models](https://github.com/CodeGeneration2/Generated-code-has-been-predicted/tree/main/Generated-code-has-been-predicted).


### E-code 350M
We give [the results of 3 times code generation in the E-code 350M model](https://github.com/CodeGeneration2/Generated-code-has-been-predicted/tree/main/Generated-code-has-been-predicted/E-code%20350M).


### GPT-Neo 125M
We give [the case results of one code generation for the GPT-Neo 125M model](https://github.com/CodeGeneration2/Generated-code-has-been-predicted/tree/main/Generated-code-has-been-predicted/GPT-Neo%20125M).


### No expert group E-code 350M
We give [the case results of one code generation for the no expert group E-code 350M model](https://github.com/CodeGeneration2/Generated-code-has-been-predicted/tree/main/Generated-code-has-been-predicted/No%20expert%20group%20E-code%20350M).

